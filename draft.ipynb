{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85dff049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (1600, 16, 5, 17, 3)\n",
      "Train labels shape: (1600,)\n",
      "\n",
      "Sample[0] shape: (16, 5, 17, 3)\n",
      "Sample[0] min, max per channel: 0.0 0.9567818 0.0 1.0 0.0 0.99173987\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 1. Load toàn bộ tập train\n",
    "train_data_path  = \"./train_data.npy\"\n",
    "train_labels_path = \"./train_labels.pkl\"\n",
    "\n",
    "data = np.load(train_data_path)            # dtype: float32, shape: (N, T, P, J, C) ?\n",
    "with open(train_labels_path, \"rb\") as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "print(\"Train data shape:\", data.shape)\n",
    "# Ví dụ: (1600, 30, 3, 17, 3)\n",
    "\n",
    "print(\"Train labels shape:\", labels.shape)\n",
    "# Ví dụ: (1600,)\n",
    "\n",
    "# 2. Kiểm tra sample đơn lẻ (để chắc chắn thứ tự axes)\n",
    "sample = data[0]\n",
    "print(\"\\nSample[0] shape:\", sample.shape)\n",
    "# => (T, P, J, C)\n",
    "\n",
    "# 3. In thử một vài giá trị min/max để xem scale\n",
    "print(\"Sample[0] min, max per channel:\", sample[...,0].min(), sample[...,0].max(),\n",
    "      sample[...,1].min(), sample[...,1].max(),\n",
    "      sample[...,2].min(), sample[...,2].max())\n",
    "\n",
    "# 4. Nếu thấy axes sai (ví dụ shape=(T, P, C, J) hoặc (T, J, P, C)), hãy thử hoán permute\n",
    "#    và xem shape mới:\n",
    "# permuted = sample.transpose(0, 2, 3, 1)\n",
    "# print(\"After transpose(0,2,3,1):\", permuted.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff54fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eda6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "matrix_random = torch.rand(8, 3, 256, 256)\n",
    "b, c, h, w = matrix_random.shape\n",
    "pw, ph = 16, 16\n",
    "nh = h // ph\n",
    "nw = w // pw\n",
    "matrix_random = torch.reshape(matrix_random, (b, c, nh, ph, nw, pw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "107c4a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 16, 16, 16, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_random.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f508c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước đầu vào văn bản (text_token_ids): torch.Size([4, 20])\n",
      "Kích thước đầu vào hình ảnh (image_vector): torch.Size([4, 768])\n",
      "Kích thước đầu ra biểu diễn đa phương thức: torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ffn_hidden_dim):\n",
    "        super().__init__()\n",
    "        # Self-Attention (SA)\n",
    "        # kdim và vdim không cần thiết ở đây vì Query, Key, Value đều cùng chiều\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Cross-Attention (CA)\n",
    "        # embed_dim là chiều đầu ra của CA (chiều của Query)\n",
    "        # kdim và vdim là chiều của Key và Value (chiều của image_vector sau khi chiếu nếu cần)\n",
    "        self.cross_attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=num_heads,\n",
    "            kdim=embed_dim, # image_vector sẽ được chiếu về embed_dim\n",
    "            vdim=embed_dim, # image_vector sẽ được chiếu về embed_dim\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Feed-Forward Network (FFN)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ffn_hidden_dim),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(ffn_hidden_dim, embed_dim)\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, text_features, image_vector):\n",
    "        # text_features: (batch_size, seq_len, embed_dim)\n",
    "        # image_vector: (batch_size, embed_dim) - đã được chiếu/xử lý để khớp chiều\n",
    "\n",
    "        # 1. Self-Attention (SA)\n",
    "        # Q, K, V đều là text_features\n",
    "        sa_output, _ = self.self_attention(\n",
    "            query=self.norm1(text_features), \n",
    "            key=self.norm1(text_features), \n",
    "            value=self.norm1(text_features)\n",
    "        )\n",
    "        text_features = text_features + sa_output \n",
    "        # \n",
    "\n",
    "        # 2. Cross-Attention (CA)\n",
    "        # Q = text_features (từ văn bản)\n",
    "        # K, V = image_vector (từ ảnh, cần mở rộng chiều)\n",
    "        # image_vector.unsqueeze(1) biến (batch_size, embed_dim) thành (batch_size, 1, embed_dim)\n",
    "        ca_output, _ = self.cross_attention(\n",
    "            query=self.norm2(text_features), \n",
    "            key=self.norm2(image_vector.unsqueeze(1)), \n",
    "            value=self.norm2(image_vector.unsqueeze(1))\n",
    "        )\n",
    "        text_features = text_features + ca_output \n",
    "        # \n",
    "\n",
    "        # 3. Feed-Forward Network (FFN)\n",
    "        ffn_output = self.feed_forward(self.norm3(text_features))\n",
    "        text_features = text_features + ffn_output\n",
    "        \n",
    "        return text_features\n",
    "\n",
    "class ImageGroundedTextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ffn_hidden_dim, num_layers, img_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # Token đặc biệt [Encode]\n",
    "        # Giả định ID của token [Encode] là 0 trong token_ids tổng quát\n",
    "        self.encode_token_id = 0 \n",
    "        self.encode_token_embedding = nn.Embedding(1, embed_dim) # Một embedding riêng cho token này\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ffn_hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Lớp chiếu để đảm bảo chiều của image_vector khớp với embed_dim của text\n",
    "        if img_hidden_dim != embed_dim:\n",
    "            self.image_proj = nn.Linear(img_hidden_dim, embed_dim)\n",
    "        else:\n",
    "            self.image_proj = nn.Identity() # Nếu chiều đã khớp thì không làm gì\n",
    "\n",
    "    def forward(self, text_token_ids, image_vector):\n",
    "        # text_token_ids: (batch_size, seq_len - 1) - không bao gồm token [Encode]\n",
    "        # image_vector: (batch_size, img_hidden_dim)\n",
    "\n",
    "        # 1. Nhúng các token văn bản\n",
    "        text_embeddings = self.token_embedding(text_token_ids)\n",
    "        \n",
    "        # 2. Tạo và thêm embedding của token [Encode] vào cuối chuỗi\n",
    "        batch_size = text_token_ids.size(0)\n",
    "        # Tạo tensor ID cho token [Encode] và nhúng nó\n",
    "        encode_token_id_tensor = torch.tensor([self.encode_token_id], device=text_token_ids.device)\n",
    "        encode_embedding = self.encode_token_embedding(encode_token_id_tensor).unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        \n",
    "        # Nối embedding của văn bản và token [Encode]\n",
    "        combined_embeddings = torch.cat((text_embeddings, encode_embedding), dim=1) \n",
    "\n",
    "        # 3. Xử lý vector ảnh để khớp chiều (nếu cần)\n",
    "        processed_image_vector = self.image_proj(image_vector)\n",
    "\n",
    "        # 4. Chạy qua các lớp Transformer\n",
    "        text_features = combined_embeddings\n",
    "        for block in self.transformer_blocks:\n",
    "            text_features = block(text_features, processed_image_vector)\n",
    "        \n",
    "        # 5. Lấy embedding của token [Encode] làm biểu diễn đa phương thức\n",
    "        multimodal_representation = text_features[:, -1, :] \n",
    "\n",
    "        return multimodal_representation\n",
    "\n",
    "# --- Ví dụ sử dụng và kiểm tra ---\n",
    "if __name__ == \"__main__\":\n",
    "    vocab_size = 10000     \n",
    "    embed_dim = 256        \n",
    "    num_heads = 8          \n",
    "    ffn_hidden_dim = 512   \n",
    "    num_layers = 2         \n",
    "    img_hidden_dim = 768   \n",
    "\n",
    "    model = ImageGroundedTextEncoder(vocab_size, embed_dim, num_heads, ffn_hidden_dim, num_layers, img_hidden_dim)\n",
    "    \n",
    "    batch_size = 4\n",
    "    seq_len = 20 \n",
    "\n",
    "    text_token_ids = torch.randint(1, vocab_size, (batch_size, seq_len)) \n",
    "    image_vector = torch.randn(batch_size, img_hidden_dim) \n",
    "\n",
    "    print(f\"Kích thước đầu vào văn bản (text_token_ids): {text_token_ids.shape}\")\n",
    "    print(f\"Kích thước đầu vào hình ảnh (image_vector): {image_vector.shape}\")\n",
    "\n",
    "    multimodal_output = model(text_token_ids, image_vector)\n",
    "\n",
    "    print(f\"Kích thước đầu ra biểu diễn đa phương thức: {multimodal_output.shape}\") \n",
    "    # Mong đợi: (batch_size, embed_dim), tức là (4, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a35620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# --- Giả lập Positional Encoding cho đơn giản ---\n",
    "# Trong thực tế, bạn sẽ dùng một implement phức tạp hơn.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x có shape [seq_len, batch_size, embedding_dim]\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x\n",
    "\n",
    "# --- Khối Decoder cơ bản (Module 3) ---\n",
    "# Mỗi khối này chứa: Causal Self-Attention -> Cross-Attention -> Feed Forward\n",
    "class ImageGroundedDecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # 1. Lớp Causal Self-Attention\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 2. Lớp Cross-Attention\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # 3. Feed Forward Network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Các lớp chuẩn hóa và dropout\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, is_causal=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tgt (Tensor): Chuỗi văn bản đầu vào. Shape: [batch_size, tgt_len, embed_dim]\n",
    "            memory (Tensor): Vector đặc trưng của ảnh. Shape: [batch_size, 1, embed_dim]\n",
    "            tgt_mask (Tensor): Mask cho self-attention.\n",
    "            is_causal (bool): Cờ để bật/tắt Causal Attention tự động.\n",
    "        \"\"\"\n",
    "        # --- Bước 1: Causal Self-Attention ---\n",
    "        # Query, Key, Value đều là `tgt`.\n",
    "        # Sử dụng cờ is_causal=True để Pytorch tự tạo mask.\n",
    "        attn_output, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask, is_causal=is_causal)\n",
    "        # Residual connection và Layer Norm\n",
    "        tgt = self.norm1(tgt + self.dropout(attn_output))\n",
    "\n",
    "        # --- Bước 2: Cross-Attention ---\n",
    "        # Query là `tgt` (kết quả từ bước trên), Key và Value là `memory` (từ ảnh).\n",
    "        # Không cần mask ở đây.\n",
    "        attn_output, _ = self.cross_attn(query=tgt, key=memory, value=memory)\n",
    "        # Residual connection và Layer Norm\n",
    "        tgt = self.norm2(tgt + self.dropout(attn_output))\n",
    "\n",
    "        # --- Bước 3: Feed Forward Network ---\n",
    "        ffn_output = self.ffn(tgt)\n",
    "        # Residual connection và Layer Norm\n",
    "        tgt = self.norm3(tgt + self.dropout(ffn_output))\n",
    "        \n",
    "        return tgt\n",
    "\n",
    "# --- Toàn bộ mô hình Decoder ---\n",
    "class ImageGroundedDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        # Bỏ qua PositionalEncoding đơn giản nếu không cần thiết cho ví dụ\n",
    "        # self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Tạo một chuỗi các Decoder Blocks\n",
    "        self.layers = nn.ModuleList([\n",
    "            ImageGroundedDecoderBlock(embed_dim, num_heads, dim_feedforward) \n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Lớp Linear cuối cùng để map ra không gian từ vựng\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, image_features, tgt_tokens):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_features (Tensor): Đặc trưng ảnh. Shape [batch_size, embed_dim]\n",
    "            tgt_tokens (Tensor): Chuỗi token văn bản. Shape [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        # 1. Embedding và Positional Encoding cho văn bản\n",
    "        # Chuyển shape thành [batch_size, seq_len, embed_dim]\n",
    "        tgt_embed = self.token_embedding(tgt_tokens) * math.sqrt(self.embed_dim)\n",
    "        # tgt_embed = self.pos_encoder(tgt_embed.permute(1,0,2)).permute(1,0,2) # Nếu dùng PositionalEncoding\n",
    "        \n",
    "        # 2. Định dạng lại vector ảnh để phù hợp với MultiheadAttention\n",
    "        # `memory` cần có shape [batch_size, num_img_tokens, embed_dim].\n",
    "        # Vì ta giả định ảnh chỉ là 1 vector, nên num_img_tokens = 1.\n",
    "        memory = image_features.unsqueeze(1) # Shape: [batch_size, 1, embed_dim]\n",
    "        \n",
    "        # 3. Đưa qua các lớp DecoderBlock\n",
    "        # Bật cờ is_causal trong forward pass của block đầu tiên hoặc của tất cả\n",
    "        # để đảm bảo tính nhân quả.\n",
    "        output = tgt_embed\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, memory, is_causal=True)\n",
    "            \n",
    "        # 4. Đưa qua lớp Linear cuối để có được logits\n",
    "        logits = self.fc_out(output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# --- Ví dụ sử dụng ---\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 4\n",
    "    SEQ_LENGTH = 15  # Độ dài chuỗi caption\n",
    "    VOCAB_SIZE = 1000 # Kích thước từ vựng\n",
    "    EMBED_DIM = 512  # Kích thước embedding (phải chia hết cho num_heads)\n",
    "    NUM_HEADS = 8    # Số lượng attention heads\n",
    "    NUM_LAYERS = 6   # Số lớp decoder\n",
    "    DIM_FFN = 2048   # Chiều của lớp ẩn trong FFN\n",
    "\n",
    "    # Khởi tạo model\n",
    "    decoder = ImageGroundedDecoder(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dim_feedforward=DIM_FFN\n",
    "    )\n",
    "    print(decoder)\n",
    "\n",
    "    # --- Tạo dữ liệu giả lập ---\n",
    "    # 1. Vector đặc trưng của ảnh (đầu ra của ViT đã được đơn giản hóa)\n",
    "    # Shape: [batch_size, embed_dim]\n",
    "    anh_feature = torch.randn(BATCH_SIZE, EMBED_DIM)\n",
    "\n",
    "    # 2. Chuỗi văn bản đầu vào cho decoder (ví dụ: \"A cat sitting on...\")\n",
    "    # Thường bắt đầu bằng token [Decode] hoặc [SOS]\n",
    "    # Shape: [batch_size, seq_len]\n",
    "    chuoi_van_ban = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))\n",
    "\n",
    "    # --- Forward pass ---\n",
    "    logits = decoder(anh_feature, chuoi_van_ban)\n",
    "\n",
    "    print(f\"\\nShape của vector ảnh đầu vào: {anh_feature.shape}\")\n",
    "    print(f\"Shape của chuỗi văn bản đầu vào: {chuoi_van_ban.shape}\")\n",
    "    print(f\"Shape của logits đầu ra: {logits.shape}\") # Mong đợi [batch_size, seq_len, vocab_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fb71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# ===================================================================\n",
    "# ===== CÁC MODULE CƠ BẢN VÀ THÀNH PHẦN GIẢ LẬP =====================\n",
    "# ===================================================================\n",
    "\n",
    "class SimpleImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Module giả lập cho Visual Transformer (ViT).\n",
    "    Nó nhận một \"ảnh\" giả và biến nó thành một chuỗi các vector đặc trưng.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_size=224, patch_size=32, in_channels=3, embed_dim=512):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_embedding = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        # Trong một ViT thực sự, sẽ có các lớp Transformer Encoder ở đây\n",
    "        self.transformer_encoder = nn.Identity() # Bỏ qua để đơn giản hóa\n",
    "\n",
    "    def forward(self, img):\n",
    "        # img shape: [batch_size, 3, 224, 224]\n",
    "        x = self.patch_embedding(img).flatten(2).transpose(1, 2) # Shape: [batch_size, num_patches, embed_dim]\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(img.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # Shape: [batch_size, num_patches + 1, embed_dim]\n",
    "        \n",
    "        x += self.positional_embedding\n",
    "        x = self.transformer_encoder(x)\n",
    "        return x # Trả về chuỗi các patch embedding, bao gồm cả [CLS] token\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # Lớp Positional Encoding chuẩn\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x có shape [batch_size, seq_len, embedding_dim]\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# ===================================================================\n",
    "# ===== MODULE 1: UNIMODAL ENCODER ==================================\n",
    "# ===================================================================\n",
    "\n",
    "class UnimodalTextEncoder(nn.Module):\n",
    "    \"\"\"Mã hóa văn bản một cách độc lập, tương tự như BERT.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True # Rất quan trọng!\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, text_tokens):\n",
    "        # text_tokens shape: [batch_size, seq_len]\n",
    "        x = self.token_embedding(text_tokens) * math.sqrt(self.embed_dim)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        encoded_text = self.transformer_encoder(x)\n",
    "        \n",
    "        # Lấy vector của [CLS] token (giả sử nó luôn ở vị trí 0)\n",
    "        cls_output = encoded_text[:, 0, :]\n",
    "        \n",
    "        return encoded_text, cls_output\n",
    "\n",
    "# ===================================================================\n",
    "# ===== MODULE 2: IMAGE-GROUNDED TEXT ENCODER =======================\n",
    "# ===================================================================\n",
    "\n",
    "class GroundedEncoderBlock(nn.Module):\n",
    "    \"\"\"Block cho Module 2: Self-Attention -> Cross-Attention -> FFN\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, txt, img_memory):\n",
    "        # 1. Self-Attention (văn bản tự chú ý đến chính nó)\n",
    "        txt2, _ = self.self_attn(txt, txt, txt)\n",
    "        txt = self.norm1(txt + self.dropout(txt2))\n",
    "        \n",
    "        # 2. Cross-Attention (văn bản chú ý đến ảnh)\n",
    "        txt2, _ = self.cross_attn(query=txt, key=img_memory, value=img_memory)\n",
    "        txt = self.norm2(txt + self.dropout(txt2))\n",
    "        \n",
    "        # 3. FFN\n",
    "        txt2 = self.ffn(txt)\n",
    "        txt = self.norm3(txt + self.dropout(txt2))\n",
    "        return txt\n",
    "\n",
    "class ImageGroundedTextEncoder(nn.Module):\n",
    "    \"\"\"Mã hóa văn bản có điều kiện là ảnh.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GroundedEncoderBlock(embed_dim, num_heads, dim_feedforward) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, text_tokens, image_features):\n",
    "        # text_tokens shape: [batch_size, seq_len]\n",
    "        # image_features shape: [batch_size, num_patches, embed_dim]\n",
    "        txt_embed = self.token_embedding(text_tokens) * math.sqrt(self.embed_dim)\n",
    "        txt_embed = self.pos_encoder(txt_embed)\n",
    "\n",
    "        output = txt_embed\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, image_features)\n",
    "            \n",
    "        # Lấy vector của [Encode] token (giả sử nó ở vị trí cuối cùng)\n",
    "        encode_output = output[:, -1, :]\n",
    "        return output, encode_output\n",
    "\n",
    "# ===================================================================\n",
    "# ===== MODULE 3: IMAGE-GROUNDED TEXT DECODER =======================\n",
    "# ===================================================================\n",
    "\n",
    "class GroundedDecoderBlock(nn.Module):\n",
    "    \"\"\"Block cho Module 3: Causal Self-Attention -> Cross-Attention -> FFN\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dim_feedforward, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, dim_feedforward), nn.ReLU(), nn.Dropout(dropout),\n",
    "            nn.Linear(dim_feedforward, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, img_memory):\n",
    "        # 1. Causal Self-Attention (dùng cờ is_causal)\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, is_causal=True)\n",
    "        tgt = self.norm1(tgt + self.dropout(tgt2))\n",
    "        \n",
    "        # 2. Cross-Attention (văn bản chú ý đến ảnh)\n",
    "        tgt2, _ = self.cross_attn(query=tgt, key=img_memory, value=img_memory)\n",
    "        tgt = self.norm2(tgt + self.dropout(tgt2))\n",
    "        \n",
    "        # 3. FFN\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(tgt2))\n",
    "        return tgt\n",
    "\n",
    "class ImageGroundedTextDecoder(nn.Module):\n",
    "    \"\"\"Sinh văn bản có điều kiện là ảnh.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embed_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            GroundedDecoderBlock(embed_dim, num_heads, dim_feedforward) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, tgt_tokens, image_features):\n",
    "        # tgt_tokens shape: [batch_size, seq_len]\n",
    "        # image_features shape: [batch_size, num_patches, embed_dim]\n",
    "        tgt_embed = self.token_embedding(tgt_tokens) * math.sqrt(self.embed_dim)\n",
    "        tgt_embed = self.pos_encoder(tgt_embed)\n",
    "\n",
    "        output = tgt_embed\n",
    "        for layer in self.layers:\n",
    "            output = layer(output, image_features)\n",
    "            \n",
    "        logits = self.fc_out(output)\n",
    "        return logits\n",
    "\n",
    "# ===================================================================\n",
    "# ===== MÔ HÌNH HỢP NHẤT MED ========================================\n",
    "# ===================================================================\n",
    "\n",
    "class MED(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dim_feedforward):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Khởi tạo các thành phần con\n",
    "        self.image_encoder = SimpleImageEncoder(embed_dim=embed_dim)\n",
    "        self.unimodal_text_encoder = UnimodalTextEncoder(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward)\n",
    "        self.grounded_encoder = ImageGroundedTextEncoder(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward)\n",
    "        self.decoder = ImageGroundedTextDecoder(vocab_size, embed_dim, num_heads, num_layers, dim_feedforward)\n",
    "        \n",
    "        # Chú ý: Trong một mô hình thực tế, bạn có thể chia sẻ trọng số,\n",
    "        # ví dụ như self.token_embedding, giữa các module.\n",
    "        # Ở đây, để rõ ràng, chúng được khởi tạo riêng biệt.\n",
    "\n",
    "    def forward(self, mode, **kwargs):\n",
    "        if mode == 'unimodal_image':\n",
    "            return self.image_encoder(kwargs['image'])\n",
    "        elif mode == 'unimodal_text':\n",
    "            return self.unimodal_text_encoder(kwargs['text_tokens'])\n",
    "        elif mode == 'grounded_encoder':\n",
    "            image_features = self.image_encoder(kwargs['image'])\n",
    "            return self.grounded_encoder(kwargs['text_tokens'], image_features)\n",
    "        elif mode == 'decoder':\n",
    "            image_features = self.image_encoder(kwargs['image'])\n",
    "            return self.decoder(kwargs['text_tokens'], image_features)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "# ===================================================================\n",
    "# ===== VÍ DỤ SỬ DỤNG ===============================================\n",
    "# ===================================================================\n",
    "if __name__ == '__main__':\n",
    "    # Hyperparameters\n",
    "    BATCH_SIZE = 4\n",
    "    SEQ_LENGTH = 20\n",
    "    VOCAB_SIZE = 1000\n",
    "    EMBED_DIM = 512\n",
    "    NUM_HEADS = 8\n",
    "    NUM_LAYERS = 6\n",
    "    DIM_FFN = 2048\n",
    "\n",
    "    # Khởi tạo mô hình hợp nhất\n",
    "    med_model = MED(\n",
    "        vocab_size=VOCAB_SIZE,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        num_heads=NUM_HEADS,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        dim_feedforward=DIM_FFN\n",
    "    )\n",
    "\n",
    "    # --- Tạo dữ liệu giả lập ---\n",
    "    dummy_image = torch.randn(BATCH_SIZE, 3, 224, 224)\n",
    "    dummy_text = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, SEQ_LENGTH))\n",
    "\n",
    "    print(\"================ CHẠY THỬ CÁC MODULE ================\")\n",
    "\n",
    "    # --- 1. Chế độ Unimodal ---\n",
    "    print(\"\\n--- 1. Chế độ Unimodal ---\")\n",
    "    img_features_unimodal = med_model(mode='unimodal_image', image=dummy_image)\n",
    "    print(f\"Image Encoder Output Shape: {img_features_unimodal.shape}\")\n",
    "    \n",
    "    encoded_text_unimodal, cls_out = med_model(mode='unimodal_text', text_tokens=dummy_text)\n",
    "    print(f\"Text Encoder Output Shape: {encoded_text_unimodal.shape}\")\n",
    "    print(f\"Text Encoder [CLS] Output Shape: {cls_out.shape}\")\n",
    "\n",
    "    # --- 2. Chế độ Image-grounded Encoder ---\n",
    "    print(\"\\n--- 2. Chế độ Image-grounded Encoder (Understanding) ---\")\n",
    "    # Giả sử token [Encode] được thêm vào cuối\n",
    "    encoded_grounded, encode_out = med_model(mode='grounded_encoder', image=dummy_image, text_tokens=dummy_text)\n",
    "    print(f\"Grounded Encoder Output Shape: {encoded_grounded.shape}\")\n",
    "    print(f\"Grounded Encoder [Encode] Output Shape: {encode_out.shape}\")\n",
    "\n",
    "    # --- 3. Chế độ Image-grounded Decoder ---\n",
    "    print(\"\\n--- 3. Chế độ Image-grounded Decoder (Generation) ---\")\n",
    "    # Giả sử chuỗi đầu vào cho decoder là `dummy_text`\n",
    "    logits = med_model(mode='decoder', image=dummy_image, text_tokens=dummy_text)\n",
    "    print(f\"Decoder Logits Output Shape: {logits.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
