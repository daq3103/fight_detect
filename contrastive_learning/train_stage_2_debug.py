# train_stage_2_supervised.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from data.dataset import SupervisedVideoDataset, collate_fn
from models.model_3dcnn import FightDetector3DCNN
from utils.trainer import SupervisedTrainer
from utils.callbacks import EarlyStopping
from torchvision.transforms import v2 as T
import random
from configs.default_config import (
    STAGE2_SUPERVISED_CONFIG as config,
    DEVICE,
    CLASSES_LIST,
    IMAGE_HEIGHT,
    IMAGE_WIDTH,
)
from configs.default_config import SEQUENCE_LENGTH_S2

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt cho vi·ªác ti·ªÅn x·ª≠ l√Ω
import os
import glob
import numpy as np
from tqdm import tqdm

# Import h√†m frames_extraction t·ª´ data_utils ƒë·ªÉ s·ª≠ d·ª•ng trong h√†m ti·ªÅn x·ª≠ l√Ω
from data.data_utils import frames_extraction


def temporal_crop_fn(vid, ratio=0.8):
    T0 = vid.size(0)
    L = max(1, int(T0 * ratio))
    start = random.randint(0, T0 - L)
    return vid[start : start + L]


def pre_process_and_save(data_dir, output_dir, classes_list, sequence_length):
    """
    H√†m ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† l∆∞u frames d∆∞·ªõi d·∫°ng t·ªáp .npy.
    N·∫øu d·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i, s·∫Ω b·ªè qua ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian.
    """
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)

    is_processed_all = True

    print(f"--- Ki·ªÉm tra v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu t·∫°i: {data_dir} ---")
    for class_name in tqdm(classes_list, desc="Processing classes"):
        class_path = os.path.join(data_dir, class_name)
        output_class_path = os.path.join(output_dir, class_name)

        if not os.path.exists(output_class_path):
            os.makedirs(output_class_path, exist_ok=True)

        for ext in ("*.avi", "*.mp4"):
            video_paths = glob.glob(os.path.join(class_path, ext))
            for video_path in tqdm(
                video_paths, desc=f"  Processing videos in '{class_name}'"
            ):
                video_name = os.path.splitext(os.path.basename(video_path))[0]
                output_file_path = os.path.join(output_class_path, f"{video_name}.npy")

                # Ki·ªÉm tra xem t·ªáp ƒë√£ t·ªìn t·∫°i ch∆∞a ƒë·ªÉ tr√°nh x·ª≠ l√Ω l·∫°i
                if os.path.exists(output_file_path):
                    continue

                is_processed_all = False
                frames = frames_extraction(
                    video_path, IMAGE_HEIGHT, IMAGE_WIDTH, sequence_length
                )

                if frames is not None:
                    # L∆∞u frames d∆∞·ªõi d·∫°ng m·∫£ng NumPy
                    np.save(output_file_path, frames)

    if not is_processed_all:
        print(f"Ti·ªÅn x·ª≠ l√Ω ho√†n t·∫•t cho th∆∞ m·ª•c: {output_dir}")
    else:
        print(f"D·ªØ li·ªáu t·∫°i {output_dir} ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω tr∆∞·ªõc ƒë√≥.")

    return not is_processed_all  # Tr·∫£ v·ªÅ True n·∫øu c√≥ t·ªáp m·ªõi ƒë∆∞·ª£c x·ª≠ l√Ω


# train_stage_2_supervised_fixed.py

def debug_model_outputs(model, train_loader, device):
    """Debug function ƒë·ªÉ ki·ªÉm tra model outputs"""
    model.eval()
    with torch.no_grad():
        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            outputs = model(inputs)
            
            print(f"Input shape: {inputs.shape}")
            print(f"Output shape: {outputs.shape}")
            print(f"Output range: [{outputs.min().item():.4f}, {outputs.max().item():.4f}]")
            print(f"Output mean: {outputs.mean().item():.4f}")
            print(f"Output std: {outputs.std().item():.4f}")
            print(f"Contains NaN: {torch.isnan(outputs).any()}")
            print(f"Contains Inf: {torch.isinf(outputs).any()}")
            break
    model.train()

# def main():
#     print("--- B·∫Øt ƒë·∫ßu Giai ƒëo·∫°n 2: Supervised Fine-tuning (FIXED) ---")

#     # ... existing preprocessing code ...

#     # 0. Augmentation - GI·∫¢M AUGMENTATION ƒë·ªÉ debug

 
#     print(f"ƒê√£ t·∫£i {len(train_dataset)} m·∫´u train v√† {len(val_dataset)} m·∫´u val.")

#     # 2. Model - KH·ªûI T·∫†O L·∫†I T·ª™ SCRATCH
#     model = FightDetector3DCNN(
#         num_classes=len(CLASSES_LIST),
#         dropout_prob=0.3,  # Gi·∫£m dropout xu·ªëng
#     )
    
#     # KI·ªÇM TRA VI·ªÜC LOAD WEIGHTS
#     try:
#         print(f"ƒêang t·∫£i tr·ªçng s·ªë t·ª´: {config['stage1_best_model_path']}")
#         checkpoint = torch.load(config["stage1_best_model_path"], map_location=DEVICE)
        
#         # In ra keys ƒë·ªÉ debug
#         print("Keys in checkpoint:")
#         for key in list(checkpoint.keys())[:10]:  # In 10 keys ƒë·∫ßu
#             print(f"  {key}: {checkpoint[key].shape if hasattr(checkpoint[key], 'shape') else type(checkpoint[key])}")
        
#         # Load v·ªõi strict=False v√† b·∫Øt l·ªói
#         missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)
#         print(f"Missing keys: {len(missing_keys)}")
#         print(f"Unexpected keys: {len(unexpected_keys)}")
        
#         if missing_keys:
#             print("Missing keys:", missing_keys[:5])  # In 5 keys ƒë·∫ßu
#         if unexpected_keys:
#             print("Unexpected keys:", unexpected_keys[:5])
            
#     except Exception as e:
#         print(f"‚ùå L·ªói khi load pretrained weights: {e}")
#         print("üîÑ Kh·ªüi t·∫°o model t·ª´ ƒë·∫ßu...")
#         # Kh·ªüi t·∫°o l·∫°i model v·ªõi pretrained ImageNet
#         model.backbone = r2plus1d_18(weights=R2Plus1D_18_Weights.KINETICS400_V1)
#         model.backbone.fc = nn.Identity()

#     if torch.cuda.device_count() > 1:
#         print(f"S·ª≠ d·ª•ng {torch.cuda.device_count()} GPU!")
#         model = nn.DataParallel(model)

#     # DEBUG MODEL OUTPUTS
#     print("\n=== DEBUGGING MODEL OUTPUTS ===")
#     debug_model_outputs(model, train_loader, DEVICE)
    
#     # 3. OPTIMIZER V·ªöI LR CAO H·ªûN V√Ä SCHEDULER ƒê∆†N GI·∫¢N
#     optimizer = optim.AdamW(
#         model.parameters(), 
#         lr=1e-4,  # LR cao h∆°n nhi·ªÅu
#         weight_decay=1e-4
#     )

def main():
    print("--- B·∫Øt ƒë·∫ßu Giai ƒëo·∫°n 2: Supervised Fine-tuning ---")

    # X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n cho d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω.
    # Thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n l∆∞u tr·ªØ ƒë·ªÉ tr·ªè ƒë·∫øn th∆∞ m·ª•c l√†m vi·ªác c·ªßa Kaggle.
    # Th∆∞ m·ª•c /kaggle/working c√≥ quy·ªÅn ghi.
    processed_data_path = f"/kaggle/working/data_processed"
    train_processed_path = os.path.join(processed_data_path, "train")
    val_processed_path = os.path.join(processed_data_path, "val")

    # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu tr∆∞·ªõc khi hu·∫•n luy·ªán
    # ... (ph·∫ßn code n√†y kh√¥ng c·∫ßn thay ƒë·ªïi) ...
    pre_process_and_save(
        data_dir=os.path.join(config["data_path"], "train"),
        output_dir=train_processed_path,
        classes_list=CLASSES_LIST,
        sequence_length=SEQUENCE_LENGTH_S2,
    )
    pre_process_and_save(
        data_dir=os.path.join(config["data_path"], "val"),
        output_dir=val_processed_path,
        classes_list=CLASSES_LIST,
        sequence_length=SEQUENCE_LENGTH_S2,
    )
    # 0. Augmentation
    transform = T.Compose([
        T.ToDtype(torch.float32, scale=True),
        T.Normalize(
            mean=[0.43216, 0.394666, 0.37645], 
            std=[0.22803, 0.22145, 0.216989]
        ),
        # T·∫°m th·ªùi t·∫Øt augmentation ƒë·ªÉ debug
        # T.RandomResizedCrop(...),
        # T.RandomHorizontalFlip(...),
    ])

    val_transform = T.Compose([
        T.ToDtype(torch.float32, scale=True),
        T.Normalize(
            mean=[0.43216, 0.394666, 0.37645], 
            std=[0.22803, 0.22145, 0.216989]
        ),
    ])


    # 1. DataLoader
    # V·∫´n s·ª≠ d·ª•ng ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu ƒë√£ ti·ªÅn x·ª≠ l√Ω
    train_dataset = SupervisedVideoDataset(
        data_dir=train_processed_path,
        classes_list=CLASSES_LIST,
        sequence_length=SEQUENCE_LENGTH_S2,
        image_height=IMAGE_HEIGHT,
        image_width=IMAGE_WIDTH,
        transform=transform,
    )

    val_dataset = SupervisedVideoDataset(
        data_dir=val_processed_path,
        classes_list=CLASSES_LIST,
        sequence_length=SEQUENCE_LENGTH_S2,
        image_height=IMAGE_HEIGHT,
        image_width=IMAGE_WIDTH,
        transform=val_transform,
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config["batch_size"],
        shuffle=True,
        num_workers=4,
        pin_memory=True,
        # collate_fn=collate_fn,
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config["batch_size"],
        shuffle=False,
        num_workers=4,
        pin_memory=True,
        collate_fn=collate_fn,
    )
    print(f"ƒê√£ t·∫£i {len(train_dataset)} m·∫´u train v√† {len(val_dataset)} m·∫´u val.")

    # 2. Model
    model = FightDetector3DCNN(
        num_classes=len(CLASSES_LIST),
        dropout_prob=0.3,  # Gi·∫£m dropout xu·ªëng
    )
    

    if torch.cuda.device_count() > 1:
        print(f"S·ª≠ d·ª•ng {torch.cuda.device_count()} GPU!")
        model = nn.DataParallel(model, device_ids=[0, 1])

    optimizer = optim.AdamW(
        model.parameters(), 
        lr=1e-4,  # LR cao h∆°n nhi·ªÅu
        weight_decay=1e-4
    )
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, "max", factor=0.5, patience=3, verbose=True
    )
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=10,  # Restart every 10 epochs
        T_mult=2,  # Double the restart interval
        eta_min=1e-7
    )
    
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    # 5. Callbacks v·ªõi EARLY STOPPING
    # early_stopping = EarlyStopping(
    #     patience=config['es_patience'],
    #     verbose=True,
    #     delta=0,
    #     path=config['model_save_path'],
    #     monitor='val_acc',
    #     mode='max'
    # )
    early_stopping = EarlyStopping(
        patience=config['es_patience'],
        verbose=True,
        delta=0.001,
        path=config['model_save_path'],
        monitor="val_accuracy",
        restore_best_weights=True,
    )
    # 4. Trainer
    trainer = SupervisedTrainer(  # SupervisedTrainer v·∫´n d√πng l·∫°i ƒë∆∞·ª£c
        model=model,
        criterion=criterion,
        optimizer=optimizer,
        device=DEVICE,
        train_dataloader=train_loader,
        val_dataloader=val_loader,
        lr_scheduler=scheduler,
        early_stopping=early_stopping,
        config=config,
    )

    trainer.train()


if __name__ == "__main__":
    main()
