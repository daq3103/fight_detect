{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Giả sử các biến này đã được định nghĩa ở nơi khác\n",
    "MoBiLSTM_model = ...\n",
    "SEQUENCE_LENGTH = ...\n",
    "IMAGE_HEIGHT, IMAGE_WIDTH = ..., ...\n",
    "CLASSES_LIST = [...]\n",
    "\n",
    "def predict_rtsp_stream(rtsp_url, SEQUENCE_LENGTH):\n",
    "    \"\"\"\n",
    "    Đọc và xử lý luồng RTSP, sau đó hiển thị kết quả.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Kết nối tới luồng RTSP.\n",
    "    video_reader = cv2.VideoCapture(rtsp_url)\n",
    "    if not video_reader.isOpened():\n",
    "        print(f\"Lỗi: Không thể mở luồng RTSP tại {rtsp_url}\")\n",
    "        return\n",
    "\n",
    "    # Khởi tạo hàng đợi để lưu trữ các frame.\n",
    "    frames_queue = deque(maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "    # Biến lưu trữ tên lớp dự đoán.\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    while True: # Vòng lặp vô hạn để xử lý luồng liên tục\n",
    "        \n",
    "        ok, frame = video_reader.read()\n",
    "        \n",
    "        if not ok:\n",
    "            print(\"Lỗi đọc frame hoặc luồng kết thúc. Đang thử kết nối lại...\")\n",
    "            video_reader.release()\n",
    "            video_reader = cv2.VideoCapture(rtsp_url) # Cố gắng kết nối lại\n",
    "            continue\n",
    "\n",
    "        # Tiền xử lý frame.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        normalized_frame = resized_frame / 255.0\n",
    "        frames_queue.append(normalized_frame)\n",
    "\n",
    "        # Chỉ dự đoán khi hàng đợi có đủ số frame.\n",
    "        if len(frames_queue) == SEQUENCE_LENGTH:\n",
    "            # Chuyển hàng đợi thành mảng numpy và dự đoán.\n",
    "            input_data = np.expand_dims(frames_queue, axis=0)\n",
    "            predicted_labels_probabilities = MoBiLSTM_model.predict(input_data)[0]\n",
    "            \n",
    "            # Lấy lớp có xác suất cao nhất.\n",
    "            predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "            predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "        # Ghi tên lớp dự đoán lên frame gốc.\n",
    "        if predicted_class_name == \"Violence\":\n",
    "            cv2.putText(frame, predicted_class_name, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 4)\n",
    "        else:\n",
    "            cv2.putText(frame, predicted_class_name, (20, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 4)\n",
    "            \n",
    "        # Hiển thị frame.\n",
    "        cv2.imshow('RTSP Stream Inference', frame)\n",
    "\n",
    "        # Nhấn 'q' để thoát.\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "    # Dọn dẹp.\n",
    "    video_reader.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb91f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n",
    " \n",
    "SEQUENCE_LENGTH = 16\n",
    " \n",
    "CLASSES_LIST = [\"NonViolence\",\"Violence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de8fec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import MobileNet\n",
    "\n",
    "mobilenet = MobileNet(\n",
    "    input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ce4d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow \n",
    "import keras\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    " \n",
    "# from sklearn.model_selection import train_test_split\n",
    " \n",
    "from keras.layers import *\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def create_model():\n",
    " \n",
    "    model = Sequential()\n",
    "\n",
    "    ########################################################################################################################\n",
    "    \n",
    "    #Specifying Input to match features shape\n",
    "    model.add(Input(shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "    \n",
    "    # Passing mobilenet in the TimeDistributed layer to handle the sequence\n",
    "    model.add(TimeDistributed(mobilenet))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "                                    \n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    \n",
    "    lstm_fw = LSTM(units=32)\n",
    "    lstm_bw = LSTM(units=32, go_backwards = True)  \n",
    "\n",
    "    model.add(Bidirectional(lstm_fw, backward_layer = lstm_bw))\n",
    "    \n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n",
    " \n",
    "    ########################################################################################################################\n",
    " \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10d20eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_4 (TimeDist (None, 16, 2, 2, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16, 2, 2, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 16, 4096)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                1057024   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,345,826\n",
      "Trainable params: 4,323,938\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_4 (TimeDist (None, 16, 2, 2, 1024)    3228864   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 16, 2, 2, 1024)    0         \n",
      "_________________________________________________________________\n",
      "time_distributed_5 (TimeDist (None, 16, 4096)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                1057024   \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 4,345,826\n",
      "Trainable params: 4,323,938\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_b = create_model()\n",
    "model_b.summary() # In tóm tắt kiến trúc của mô hình hiện tại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a2feb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "932e1e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('trained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f53922cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Convert\n",
    "spec = (tf.TensorSpec((None, SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3), tf.float32, name=\"input\"),)\n",
    "output_path = \"model.onnx\"\n",
    "model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76644241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Extraction of attribute type 8 not implemented.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m onnx_model = onnx.load(\u001b[33m\"\u001b[39m\u001b[33m./model.onnx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Chuyển thành PyTorch, onnx2pytorch có hỗ trợ LSTM/GRU/RNN\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m pytorch_model = \u001b[43mConvertModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Kiểm tra\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(pytorch_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\model.py:122\u001b[39m, in \u001b[36mConvertModel.__init__\u001b[39m\u001b[34m(self, onnx_model, batch_dim, experimental, debug, enable_pruning)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# Create mapping from node (identified by first output) to submodule\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mself\u001b[39m.mapping = {}\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconvert_operations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43monnx_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_pruning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43msetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLoop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\operations.py:153\u001b[39m, in \u001b[36mconvert_operations\u001b[39m\u001b[34m(onnx_graph, opset_version, batch_dim, enable_pruning)\u001b[39m\n\u001b[32m    147\u001b[39m     op = Loop(\n\u001b[32m    148\u001b[39m         opset_version=opset_version,\n\u001b[32m    149\u001b[39m         batch_dim=batch_dim,\n\u001b[32m    150\u001b[39m         **extract_attributes(node),\n\u001b[32m    151\u001b[39m     )\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m node.op_type == \u001b[33m\"\u001b[39m\u001b[33mLSTM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     op = \u001b[43mconvert_lstm_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m node.op_type == \u001b[33m\"\u001b[39m\u001b[33mMatMul\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m params:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\layer.py:208\u001b[39m, in \u001b[36mconvert_lstm_layer\u001b[39m\u001b[34m(node, weights)\u001b[39m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mLSTM P not yet implemented.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m dc = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    199\u001b[39m     activation_alpha=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    200\u001b[39m     activation_beta=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    206\u001b[39m     layout=\u001b[32m0\u001b[39m,\n\u001b[32m    207\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m dc.update(\u001b[43mextract_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dc[\u001b[33m\"\u001b[39m\u001b[33mactivation_alpha\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    211\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLSTM activation_alpha \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m.format(dc[\u001b[33m\"\u001b[39m\u001b[33mactivation_alpha\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    212\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\attribute.py:62\u001b[39m, in \u001b[36mextract_attributes\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m     60\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mactivation_beta\u001b[39m\u001b[33m\"\u001b[39m] = extract_attr_values(attr)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr.name == \u001b[33m\"\u001b[39m\u001b[33mactivations\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mactivations\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mextract_attr_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m attr.name == \u001b[33m\"\u001b[39m\u001b[33malpha\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     64\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node.op_type == \u001b[33m\"\u001b[39m\u001b[33mLeakyRelu\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\minh.nhat\\AppData\\Local\\anaconda3\\envs\\natmin\\Lib\\site-packages\\onnx2pytorch\\convert\\attribute.py:47\u001b[39m, in \u001b[36mextract_attr_values\u001b[39m\u001b[34m(attr)\u001b[39m\n\u001b[32m     45\u001b[39m     value = attr.g\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExtraction of attribute type \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m not implemented.\u001b[39m\u001b[33m\"\u001b[39m.format(attr.type)\n\u001b[32m     49\u001b[39m     )\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[31mNotImplementedError\u001b[39m: Extraction of attribute type 8 not implemented."
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnx2pytorch import ConvertModel\n",
    "\n",
    "# Load ONNX\n",
    "onnx_model = onnx.load(\"./model.onnx\")\n",
    "\n",
    "# Chuyển thành PyTorch, onnx2pytorch có hỗ trợ LSTM/GRU/RNN\n",
    "pytorch_model = ConvertModel(onnx_model)\n",
    "\n",
    "# Kiểm tra\n",
    "print(pytorch_model)\n",
    "\n",
    "# Lưu lại\n",
    "import torch\n",
    "torch.save(pytorch_model, \"./pytorch_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb6a981",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
