{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c3c14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Khối xây dựng này không đổi\n",
    "class FusedTransformerBlock(nn.Module):\n",
    "    \"\"\"Một khối Transformer kết hợp cho cả Encoder và Decoder. (SA -> CA -> FFN)\"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout_rate=0.1, is_causal=False):\n",
    "        super().__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout_rate, batch_first=True)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.ffn = nn.Sequential(nn.Linear(embed_dim, ff_dim), nn.GELU(), nn.Dropout(dropout_rate), nn.Linear(ff_dim, embed_dim))\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, text_embeds, visual_embeds, text_mask=None, visual_mask=None):\n",
    "        sa_attn_mask = self._generate_causal_mask(text_embeds.size(1)).to(text_embeds.device) if self.is_causal else None\n",
    "        sa_out, _ = self.self_attn(query=text_embeds, key=text_embeds, value=text_embeds, attn_mask=sa_attn_mask, key_padding_mask=text_mask)\n",
    "        text_embeds = self.norm1(text_embeds + self.dropout1(sa_out))\n",
    "        ca_out, _ = self.cross_attn(query=text_embeds, key=visual_embeds, value=visual_embeds, key_padding_mask=visual_mask)\n",
    "        text_embeds = self.norm2(text_embeds + self.dropout2(ca_out))\n",
    "        ffn_out = self.ffn(text_embeds)\n",
    "        text_embeds = self.norm3(text_embeds + self.dropout3(ffn_out))\n",
    "        return text_embeds\n",
    "\n",
    "    def _generate_causal_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "\n",
    "# --- CÁC THÀNH PHẦN SẼ ĐƯỢC CHIA SẺ ---\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"Placeholder cho ViT, chịu trách nhiệm mã hóa ảnh.\"\"\"\n",
    "    def __init__(self, embed_dim=768, image_size=224, patch_size=16):\n",
    "        super().__init__()\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.patch_proj = nn.Linear(3 * patch_size * patch_size, embed_dim) # Placeholder\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, image_patches):\n",
    "        patch_embeds = self.patch_proj(image_patches)\n",
    "        cls_token = self.cls_token.expand(patch_embeds.shape[0], -1, -1)\n",
    "        visual_embeds = torch.cat((cls_token, patch_embeds), dim=1)\n",
    "        visual_embeds += self.pos_embed\n",
    "        return self.norm(visual_embeds)\n",
    "\n",
    "class SharedTextEmbeddings(nn.Module):\n",
    "    \"\"\"Module chứa các lớp embedding văn bản được chia sẻ.\"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=768, max_seq_len=512):\n",
    "        super().__init__()\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_seq_len, embed_dim))\n",
    "\n",
    "    def forward(self, text_tokens):\n",
    "        seq_len = text_tokens.size(1)\n",
    "        embeds = self.token_embed(text_tokens) + self.pos_embed[:, :seq_len, :]\n",
    "        return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e809243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODULE 1: UNIMODAL ENCODER ---\n",
    "class UnimodalModule(nn.Module):\n",
    "    def __init__(self, image_encoder, shared_text_embeddings, num_layers=12, embed_dim=768, num_heads=12, ff_dim=3072):\n",
    "        super().__init__()\n",
    "        # Nhận các module đã được khởi tạo từ bên ngoài\n",
    "        self.image_encoder = image_encoder\n",
    "        self.shared_text_embeddings = shared_text_embeddings\n",
    "        \n",
    "        # Tự tạo ra các thành phần của riêng nó\n",
    "        bert_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim, batch_first=True)\n",
    "        self.text_encoder = nn.TransformerEncoder(bert_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, image_patches=None, text_tokens=None, text_mask=None):\n",
    "        visual_output = self.image_encoder(image_patches) if image_patches is not None else None\n",
    "        \n",
    "        text_output = None\n",
    "        if text_tokens is not None:\n",
    "            text_embeds = self.shared_text_embeddings(text_tokens)\n",
    "            text_output = self.text_encoder(text_embeds, src_key_padding_mask=text_mask)\n",
    "            \n",
    "        return visual_output, text_output\n",
    "\n",
    "# --- MODULE 2: IMAGE-GROUNDED TEXT ENCODER ---\n",
    "class ImageGroundedEncoderModule(nn.Module):\n",
    "    def __init__(self, shared_text_embeddings, num_layers=12, embed_dim=768, num_heads=12, ff_dim=3072):\n",
    "        super().__init__()\n",
    "        self.shared_text_embeddings = shared_text_embeddings\n",
    "        self.blocks = nn.ModuleList([\n",
    "            FusedTransformerBlock(embed_dim, num_heads, ff_dim, is_causal=False) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, text_tokens, visual_embeds, text_mask=None):\n",
    "        text_embeds = self.shared_text_embeddings(text_tokens)\n",
    "        for block in self.blocks:\n",
    "            text_embeds = block(text_embeds=text_embeds, visual_embeds=visual_embeds, text_mask=text_mask)\n",
    "        return text_embeds\n",
    "\n",
    "# --- MODULE 3: IMAGE-GROUNDED TEXT DECODER ---\n",
    "class ImageGroundedDecoderModule(nn.Module):\n",
    "    def __init__(self, shared_text_embeddings, vocab_size, num_layers=12, embed_dim=768, num_heads=12, ff_dim=3072):\n",
    "        super().__init__()\n",
    "        self.shared_text_embeddings = shared_text_embeddings\n",
    "        self.blocks = nn.ModuleList([\n",
    "            FusedTransformerBlock(embed_dim, num_heads, ff_dim, is_causal=True) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.vocab_predictor = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, text_tokens, visual_embeds, text_mask=None):\n",
    "        text_embeds = self.shared_text_embeddings(text_tokens)\n",
    "        for block in self.blocks:\n",
    "            text_embeds = block(text_embeds=text_embeds, visual_embeds=visual_embeds, text_mask=text_mask)\n",
    "        return self.vocab_predictor(text_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46527e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MED_Composed(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=768, num_layers=12, num_heads=12, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Khởi tạo các thành phần DÙNG CHUNG\n",
    "        self.shared_image_encoder = ImageEncoder(embed_dim=embed_dim)\n",
    "        self.shared_text_embeddings = SharedTextEmbeddings(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "        \n",
    "        # 2. Khởi tạo các module riêng biệt và \"TIÊM\" (inject) các thành phần dùng chung vào\n",
    "        self.unimodal_module = UnimodalModule(\n",
    "            image_encoder=self.shared_image_encoder,\n",
    "            shared_text_embeddings=self.shared_text_embeddings,\n",
    "            num_layers=num_layers, embed_dim=embed_dim, num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        self.grounded_encoder_module = ImageGroundedEncoderModule(\n",
    "            shared_text_embeddings=self.shared_text_embeddings,\n",
    "            num_layers=num_layers, embed_dim=embed_dim, num_heads=num_heads\n",
    "        )\n",
    "        \n",
    "        self.grounded_decoder_module = ImageGroundedDecoderModule(\n",
    "            shared_text_embeddings=self.shared_text_embeddings,\n",
    "            vocab_size=vocab_size,\n",
    "            num_layers=num_layers, embed_dim=embed_dim, num_heads=num_heads\n",
    "        )\n",
    "\n",
    "    def forward(self, mode, image_patches=None, text_tokens=None, text_mask=None):\n",
    "        if mode == 'unimodal':\n",
    "            # Module unimodal tự quản lý image_encoder\n",
    "            return self.unimodal_module(image_patches, text_tokens, text_mask)\n",
    "            \n",
    "        elif mode in ['grounded_encoder', 'grounded_decoder']:\n",
    "            if image_patches is None or text_tokens is None:\n",
    "                raise ValueError(\"Grounded modes require both image and text inputs.\")\n",
    "            \n",
    "            # Mã hóa ảnh một lần duy nhất bằng module chia sẻ\n",
    "            visual_embeds = self.shared_image_encoder(image_patches)\n",
    "            \n",
    "            if mode == 'grounded_encoder':\n",
    "                return self.grounded_encoder_module(text_tokens, visual_embeds, text_mask)\n",
    "            else: # grounded_decoder\n",
    "                return self.grounded_decoder_module(text_tokens, visual_embeds, text_mask)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c09cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec2b28e",
   "metadata": {},
   "source": [
    "### Style 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b056008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, BertConfig\n",
    "\n",
    "# --- Helper functions/classes for better readability ---\n",
    "# (Using nn.Identity as a placeholder where actual layers would be in a full impl)\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, embed_dim=768, num_layers=12, num_heads=12):\n",
    "        super().__init__()\n",
    "        # In a real BLIP implementation, this would be a Vision Transformer (ViT).\n",
    "        # You'd typically use 'timm' library or a custom ViT implementation.\n",
    "        # For simplicity and focusing on overall architecture, we'll mimic its output.\n",
    "\n",
    "        # Mimic patch embedding and position embedding\n",
    "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Transformer Encoder layers (using standard PyTorch TransformerEncoderLayer)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=embed_dim * 4, \n",
    "            batch_first=True # Important for batch_size first\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, images):\n",
    "        x = self.patch_embedding(images).flatten(2).transpose(1, 2) # (B, N_patches, E)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (B, N_patches+1, E)\n",
    "        x += self.pos_embedding[:, :(x.shape[1])] # Add positional embedding\n",
    "\n",
    "        # Transformer Encoder expects (batch_size, sequence_length, embed_dim)\n",
    "        features = self.transformer_encoder(x)\n",
    "        return features # (B, N_patches+1, E)\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        # We leverage Hugging Face Transformers for the base BERT model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name)\n",
    "        self.config = self.bert_model.config\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask):\n",
    "        # attention_mask from tokenizer ensures padding tokens are ignored\n",
    "        outputs = self.bert_model(input_ids=text_input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state # (B, seq_len, E)\n",
    "\n",
    "# --- Custom Transformer Layer for Image-Grounded Encoder ---\n",
    "# This class specifically modifies the standard EncoderLayer to insert cross-attention\n",
    "# as per the BLIP diagram (after self-attention and before FFN).\n",
    "class ImageGroundedEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=nn.GELU()):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model) # For FFN after cross-attn\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, src, memory, src_mask=None, memory_mask=None, src_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # Self-Attention\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Cross-Attention (query from src, key/value from memory)\n",
    "        src2 = self.cross_attn(src, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src) # Norm after cross-attention\n",
    "\n",
    "        # Feed-forward\n",
    "        src3 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src3)\n",
    "        src = self.norm3(src) # Norm after FFN\n",
    "        return src\n",
    "\n",
    "class ImageGroundedTextEncoder(nn.Module):\n",
    "    def __init__(self, bert_config, num_image_grounded_layers=4):\n",
    "        super().__init__()\n",
    "        # Use a base BERT model (just its embeddings and the first few layers if needed)\n",
    "        self.bert_embedding_layer = AutoModel.from_pretrained(\"bert-base-uncased\", config=bert_config).embeddings\n",
    "        self.embed_dim = bert_config.hidden_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # For special tokens if needed\n",
    "\n",
    "        # Create custom encoder layers that include cross-attention\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            ImageGroundedEncoderLayer(self.embed_dim, bert_config.num_attention_heads, bert_config.intermediate_size, bert_config.hidden_dropout_prob)\n",
    "            for _ in range(num_image_grounded_layers)\n",
    "        ])\n",
    "        \n",
    "        # Token for [Encode] - Assuming it's added via tokenizer to input\n",
    "        # No explicit embedding for [Encode] needed if tokenizer handles it.\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask, image_features):\n",
    "        # Get embeddings from BERT's embedding layer\n",
    "        # text_input_ids should already contain the [Encode] token\n",
    "        text_embeddings = self.bert_embedding_layer(input_ids=text_input_ids)\n",
    "        \n",
    "        # Create mask for text self-attention (from attention_mask)\n",
    "        # nn.MultiheadAttention expects boolean masks: True for masked (ignored) position\n",
    "        src_key_padding_mask = (attention_mask == 0) # Convert 0s (padding) to True\n",
    "        \n",
    "        # Run through custom encoder layers, injecting image features via cross-attention\n",
    "        hidden_states = text_embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            # src is text, memory is image_features\n",
    "            hidden_states = layer(\n",
    "                hidden_states, \n",
    "                image_features, \n",
    "                src_key_padding_mask=src_key_padding_mask # Mask for text (src) self-attention\n",
    "                # No memory_key_padding_mask for image features for simplicity, assuming no padding\n",
    "            )\n",
    "        \n",
    "        # The output of this module is the representation of the [Encode] token.\n",
    "        # This typically means extracting the embedding at the position of the [Encode] token.\n",
    "        # For this example, we return all hidden_states, and the BLIP class will handle extraction.\n",
    "        return hidden_states # (B, seq_len, E)\n",
    "\n",
    "# --- ImageGroundedTextDecoder Class (for LM) ---\n",
    "class ImageGroundedTextDecoder(nn.Module):\n",
    "    def __init__(self, bert_config, num_image_grounded_layers=4):\n",
    "        super().__init__()\n",
    "        self.bert_embedding_layer = AutoModel.from_pretrained(\"bert-base-uncased\", config=bert_config).embeddings\n",
    "        self.embed_dim = bert_config.hidden_size\n",
    "\n",
    "        # PyTorch's TransformerDecoderLayer inherently has self-attention and cross-attention\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=bert_config.num_attention_heads, \n",
    "            dim_feedforward=bert_config.intermediate_size, \n",
    "            dropout=bert_config.hidden_dropout_prob,\n",
    "            activation=nn.GELU(),\n",
    "            batch_first=True # Important for batch_size first\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_image_grounded_layers)\n",
    "        \n",
    "        # Output layer for language modeling\n",
    "        self.lm_head = nn.Linear(self.embed_dim, bert_config.vocab_size)\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask, image_features):\n",
    "        text_embeddings = self.bert_embedding_layer(input_ids=text_input_ids)\n",
    "        \n",
    "        # Create causal mask for self-attention in decoder\n",
    "        # Causal mask: ensure token only attends to previous tokens\n",
    "        seq_len = text_input_ids.shape[1]\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(text_input_ids.device)\n",
    "        \n",
    "        # Create key padding mask for decoder (based on attention_mask)\n",
    "        tgt_key_padding_mask = (attention_mask == 0) # True for padding positions\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        # tgt: text embeddings, memory: image features\n",
    "        # tgt_mask: causal mask, tgt_key_padding_mask: padding mask for text\n",
    "        # memory_key_padding_mask: padding mask for image features (if any)\n",
    "        hidden_states = self.transformer_decoder(\n",
    "            tgt=text_embeddings,\n",
    "            memory=image_features,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits # (B, seq_len, vocab_size)\n",
    "\n",
    "\n",
    "# --- BLIP Main Class (Tổng hợp) ---\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self, image_encoder_params, text_encoder_params, bert_config, \n",
    "                 itm_num_layers=4, lm_num_layers=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # Khởi tạo các module cơ bản\n",
    "        self.image_encoder = ImageEncoder(**image_encoder_params)\n",
    "        \n",
    "        # TextEncoder cho Unimodal và ITC (dùng BERT gốc)\n",
    "        self.text_encoder_unimodal = TextEncoder(**text_encoder_params) \n",
    "        \n",
    "        # Image-grounded Text Encoder (cho ITM)\n",
    "        self.image_grounded_text_encoder = ImageGroundedTextEncoder(\n",
    "            bert_config=bert_config,\n",
    "            num_image_grounded_layers=itm_num_layers\n",
    "        )\n",
    "        \n",
    "        # Image-grounded Text Decoder (cho LM)\n",
    "        self.image_grounded_text_decoder = ImageGroundedTextDecoder(\n",
    "            bert_config=bert_config,\n",
    "            num_image_grounded_layers=lm_num_layers\n",
    "        )\n",
    "\n",
    "        # Các linear layers cho ITC và ITM heads\n",
    "        embed_dim = bert_config.hidden_size \n",
    "        self.itc_head = nn.Linear(embed_dim, embed_dim) # For contrastive projection\n",
    "        self.itm_head = nn.Linear(embed_dim, 2) # Binary classification for ITM (matched/unmatched)\n",
    "\n",
    "        # Loss functions\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=self.text_encoder_unimodal.tokenizer.pad_token_id)\n",
    "        # For ITC, typically uses InfoNCE Loss, which is more complex.\n",
    "        # For simplicity, I'll use a placeholder similarity loss here.\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * 0.07) # Learnable temperature for contrastive loss\n",
    "\n",
    "    # --- Các phương thức theo \"functionality\" ---\n",
    "\n",
    "    def unimodal_encode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        image_cls_feature = image_features[:, 0, :] # [CLS] token of image\n",
    "\n",
    "        text_features = self.text_encoder_unimodal(text_input_ids, text_attention_mask)\n",
    "        text_cls_feature = text_features[:, 0, :] # [CLS] token of text\n",
    "\n",
    "        return image_cls_feature, text_cls_feature\n",
    "\n",
    "    def image_grounded_encode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        \n",
    "        multimodal_representations = self.image_grounded_text_encoder(\n",
    "            text_input_ids, text_attention_mask, image_features\n",
    "        )\n",
    "        \n",
    "        # Assume [Encode] token is at the end of the sequence for its representation\n",
    "        encode_token_representation = multimodal_representations[:, -1, :] \n",
    "        \n",
    "        return encode_token_representation\n",
    "\n",
    "    def image_grounded_decode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        logits = self.image_grounded_text_decoder(\n",
    "            text_input_ids, text_attention_mask, image_features\n",
    "        )\n",
    "        return logits\n",
    "\n",
    "    def forward(self, images, \n",
    "                text_input_ids_itc=None, text_attention_mask_itc=None,\n",
    "                text_input_ids_itm=None, text_attention_mask_itm=None, itm_labels=None,\n",
    "                text_input_ids_lm=None, text_attention_mask_lm=None, lm_labels=None,\n",
    "                task_type=\"pretrain\"):\n",
    "        \n",
    "        total_loss = 0\n",
    "        losses_dict = {}\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"itc\":\n",
    "            itc_loss = self.calculate_itc_loss(images, text_input_ids_itc, text_attention_mask_itc)\n",
    "            total_loss += itc_loss\n",
    "            losses_dict[\"itc_loss\"] = itc_loss\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"itm\":\n",
    "            itm_loss = self.calculate_itm_loss(images, text_input_ids_itm, text_attention_mask_itm, itm_labels)\n",
    "            total_loss += itm_loss\n",
    "            losses_dict[\"itm_loss\"] = itm_loss\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"lm\":\n",
    "            lm_loss = self.calculate_lm_loss(images, text_input_ids_lm, text_attention_mask_lm, lm_labels)\n",
    "            total_loss += lm_loss\n",
    "            losses_dict[\"lm_loss\"] = lm_loss\n",
    "            \n",
    "        if task_type == \"pretrain\":\n",
    "            return total_loss, losses_dict\n",
    "        elif task_type in [\"itc\", \"itm\", \"lm\"]:\n",
    "            return total_loss # Return specific loss if only one task is requested\n",
    "        elif task_type == \"inference\":\n",
    "            # Implement your inference logic here (e.g., text generation, image captioning)\n",
    "            raise NotImplementedError(\"Inference logic not implemented yet.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task_type: {task_type}\")\n",
    "\n",
    "    # --- Các hàm tính Loss (Implemented using PyTorch ops) ---\n",
    "    def calculate_itc_loss(self, images, text_input_ids, text_attention_mask):\n",
    "        img_cls, text_cls = self.unimodal_encode(images, text_input_ids, text_attention_mask)\n",
    "        \n",
    "        # Project features for contrastive loss\n",
    "        img_proj = self.itc_head(img_cls)\n",
    "        text_proj = self.itc_head(text_cls)\n",
    "\n",
    "        # Normalize features\n",
    "        img_proj = torch.nn.functional.normalize(img_proj, dim=-1)\n",
    "        text_proj = torch.nn.functional.normalize(text_proj, dim=-1)\n",
    "\n",
    "        # Compute cosine similarity and apply temperature\n",
    "        # temp * CosineSimilarity(query, key)\n",
    "        logits_per_image = torch.matmul(img_proj, text_proj.T) / self.temperature\n",
    "        logits_per_text = logits_per_image.T # Transposed for text perspective\n",
    "\n",
    "        # InfoNCE Loss (symmetric cross-entropy)\n",
    "        labels = torch.arange(logits_per_image.shape[0], device=logits_per_image.device) # Diagonal is positive pair\n",
    "        loss_i = self.cross_entropy_loss(logits_per_image, labels)\n",
    "        loss_t = self.cross_entropy_loss(logits_per_text, labels)\n",
    "        \n",
    "        loss = (loss_i + loss_t) / 2\n",
    "        return loss\n",
    "\n",
    "    def calculate_itm_loss(self, images, text_input_ids, text_attention_mask, labels):\n",
    "        # Labels are 0 for unmatched, 1 for matched\n",
    "        multimodal_representation = self.image_grounded_encode(images, text_input_ids, text_attention_mask)\n",
    "        logits = self.itm_head(multimodal_representation) \n",
    "        loss = self.cross_entropy_loss(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def calculate_lm_loss(self, images, text_input_ids, text_attention_mask, labels):\n",
    "        # text_input_ids_lm should be shifted right for input to decoder\n",
    "        # labels should be the actual target tokens\n",
    "        logits = self.image_grounded_decode(images, text_input_ids, text_attention_mask)\n",
    "        \n",
    "        # For LM, we typically predict the next token.\n",
    "        # labels should be text_input_ids[:, 1:] and logits[:, :-1, :]\n",
    "        # Flatten for CrossEntropyLoss\n",
    "        loss = self.cross_entropy_loss(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return loss\n",
    "\n",
    "# --- Ví dụ sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Cấu hình các tham số cho từng module\n",
    "    image_encoder_params = {\n",
    "        \"image_size\": 224,\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": 768,\n",
    "        \"num_layers\": 12, # Number of Encoder blocks in ViT\n",
    "        \"num_heads\": 12\n",
    "    }\n",
    "    \n",
    "    text_encoder_params = {\n",
    "        \"model_name\": \"bert-base-uncased\"\n",
    "    }\n",
    "    \n",
    "    # Lấy config từ BERT để khởi tạo các lớp MED\n",
    "    # Đây là cách tốt để đảm bảo kích thước embedding, số head, v.v. nhất quán\n",
    "    bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Khởi tạo mô hình BLIP\n",
    "    blip_model = BLIP(\n",
    "        image_encoder_params=image_encoder_params,\n",
    "        text_encoder_params=text_encoder_params,\n",
    "        bert_config=bert_config,\n",
    "        itm_num_layers=4, # Number of Image-Grounded Encoder layers\n",
    "        lm_num_layers=4   # Number of Image-Grounded Decoder layers\n",
    "    ).to(device)\n",
    "    \n",
    "    blip_model.eval() # Chuyển sang chế độ evaluation\n",
    "\n",
    "    print(\"BLIP Model initialized successfully!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in blip_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "    # --- Tạo dữ liệu dummy để kiểm tra forward pass ---\n",
    "    batch_size = 2\n",
    "    dummy_images = torch.randn(batch_size, 3, 224, 224).to(device) \n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    dummy_texts = [\"a little girl holding a kitten next to a blue fence\", \"a dog running in a park\"]\n",
    "\n",
    "    # For ITC, ITM, LM, we prepare distinct inputs as per BLIP's training strategy\n",
    "    # ITC: original image-text pairs\n",
    "    encoded_itc = tokenizer(dummy_texts, padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"pt\")\n",
    "    text_input_ids_itc = encoded_itc.input_ids.to(device)\n",
    "    text_attention_mask_itc = encoded_itc.attention_mask.to(device)\n",
    "\n",
    "    # ITM: mixed positive/negative pairs. Here we simplify.\n",
    "    # [CLS] text [SEP] [ENC]\n",
    "    itm_texts = [f\"[CLS] {t} [SEP] {tokenizer.encode('[ENC]', add_special_tokens=False)[0]}\" for t in dummy_texts] # Not a real token, needs to be handled by tokenizer vocab\n",
    "    # In a real BLIP setup, [ENC] and [DEC] are added as special tokens to tokenizer's vocabulary.\n",
    "    # For dummy, let's just reuse original tokens and assume [ENC] is the last token added by some process.\n",
    "    # A more robust dummy:\n",
    "    encode_token_id = tokenizer.add_special_tokens({'additional_special_tokens': ['[ENC]']})['additional_special_tokens'][0]\n",
    "    encoded_itm_raw = tokenizer(dummy_texts, padding=True, truncation=True, max_length=49, return_tensors=\"pt\")\n",
    "    text_input_ids_itm = torch.cat([encoded_itm_raw.input_ids, torch.full((batch_size, 1), encode_token_id, dtype=torch.long)], dim=1).to(device)\n",
    "    text_attention_mask_itm = torch.cat([encoded_itm_raw.attention_mask, torch.ones((batch_size, 1), dtype=torch.long)], dim=1).to(device)\n",
    "    itm_labels = torch.tensor([1, 0], dtype=torch.long).to(device) # Dummy labels: 1 matched, 0 unmatched\n",
    "\n",
    "    # LM: auto-regressive generation.\n",
    "    # [CLS] [DEC] text_prefix\n",
    "    decode_token_id = tokenizer.add_special_tokens({'additional_special_tokens': ['[DEC]']})['additional_special_tokens'][0]\n",
    "    # For LM input: [DEC] + text_prefix\n",
    "    # For LM labels: text_prefix + [EOS] (shifted)\n",
    "    \n",
    "    # Dummy LM inputs: [DEC] + \"a little girl\" -> predict \" holding a kitten next to a blue fence\"\n",
    "    lm_input_texts = [f\"{tokenizer.decode(decode_token_id)} a little girl\", f\"{tokenizer.decode(decode_token_id)} a dog running\"]\n",
    "    lm_target_texts = [\"a little girl holding a kitten next to a blue fence\", \"a dog running in a park\"]\n",
    "\n",
    "    encoded_lm_input = tokenizer(lm_input_texts, padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"pt\")\n",
    "    text_input_ids_lm = encoded_lm_input.input_ids.to(device)\n",
    "    text_attention_mask_lm = encoded_lm_input.attention_mask.to(device)\n",
    "\n",
    "    # For labels, we need the full target sequence, shifted for prediction\n",
    "    encoded_lm_target = tokenizer(lm_target_texts, padding=\"max_length\", truncation=True, max_length=50, return_tensors=\"pt\")\n",
    "    lm_labels = encoded_lm_target.input_ids.to(device)\n",
    "    \n",
    "    # In LM loss, we typically predict token i+1 from input token i.\n",
    "    # So, labels should be the target sequence, and input is shifted.\n",
    "    # Here, `text_input_ids_lm` is input, `lm_labels` is target.\n",
    "    # Make sure padding tokens are ignored in loss.\n",
    "\n",
    "    print(\"\\n--- Testing Unimodal Encoding (for ITC) ---\")\n",
    "    with torch.no_grad():\n",
    "        img_cls, text_cls = blip_model.unimodal_encode(dummy_images, text_input_ids_itc, text_attention_mask_itc)\n",
    "    print(f\"Image CLS feature shape: {img_cls.shape}\") \n",
    "    print(f\"Text CLS feature shape: {text_cls.shape}\") \n",
    "\n",
    "    print(\"\\n--- Testing Image-Grounded Encoding (for ITM) ---\")\n",
    "    with torch.no_grad():\n",
    "        itm_output = blip_model.image_grounded_encode(dummy_images, text_input_ids_itm, text_attention_mask_itm)\n",
    "    print(f\"Image-Grounded Encode (ITM) output shape: {itm_output.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing Image-Grounded Decoding (for LM) ---\")\n",
    "    with torch.no_grad():\n",
    "        lm_logits = blip_model.image_grounded_decode(dummy_images, text_input_ids_lm, text_attention_mask_lm)\n",
    "    print(f\"Image-Grounded Decode (LM) logits shape: {lm_logits.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing Full Pretrain Forward Pass ---\")\n",
    "    # Set model to train mode to allow gradient calculation if running training\n",
    "    blip_model.train() \n",
    "    total_loss, losses_dict = blip_model.forward(\n",
    "        images=dummy_images,\n",
    "        text_input_ids_itc=text_input_ids_itc,\n",
    "        text_attention_mask_itc=text_attention_mask_itc,\n",
    "        text_input_ids_itm=text_input_ids_itm,\n",
    "        text_attention_mask_itm=text_attention_mask_itm,\n",
    "        itm_labels=itm_labels,\n",
    "        text_input_ids_lm=text_input_ids_lm,\n",
    "        text_attention_mask_lm=text_attention_mask_lm,\n",
    "        lm_labels=lm_labels,\n",
    "        task_type=\"pretrain\"\n",
    "    )\n",
    "    print(f\"Total Pretrain Loss: {total_loss.item():.4f}\")\n",
    "    print(f\"Individual Losses: {losses_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e2464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer, BertConfig\n",
    "import copy # For deepcopy for momentum encoders\n",
    "\n",
    "# --- 1. Image Encoder (Simplified Vision Transformer structure) ---\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_size=224, patch_size=16, embed_dim=768, num_layers=12, num_heads=12):\n",
    "        super().__init__()\n",
    "        # Mimic ViT's patch embedding: 3x3 input image, embed_dim output\n",
    "        # kernel_size=patch_size, stride=patch_size ensures non-overlapping patches\n",
    "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Calculate number of patches and add CLS token\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim)) # Learnable CLS token\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim)) # Learnable positional embedding\n",
    "\n",
    "        # Transformer Encoder layers\n",
    "        # Using PyTorch's built-in TransformerEncoderLayer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=embed_dim * 4, # Standard expansion in FFN\n",
    "            dropout=0.1, \n",
    "            activation=nn.GELU(),\n",
    "            batch_first=True # Important for batch_size first\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Apply patch embedding and flatten to (B, N_patches, E)\n",
    "        x = self.patch_embedding(images).flatten(2).transpose(1, 2) \n",
    "         \n",
    "        # Prepend CLS token and add positional embedding\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # (B, N_patches+1, E)\n",
    "        x += self.pos_embedding[:, :(x.shape[1])] # Add positional embedding (truncated if sequence length varies)\n",
    "        \n",
    "        # Pass through Transformer Encoder\n",
    "        features = self.transformer_encoder(x)\n",
    "        return features # (B, N_patches+1, E)\n",
    "\n",
    "# --- 2. Text Encoder (Standard BERT for Unimodal) ---\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"bert-base-uncased\"):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.bert_model = AutoModel.from_pretrained(model_name)\n",
    "        self.config = self.bert_model.config\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask):\n",
    "        outputs = self.bert_model(input_ids=text_input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state # (B, seq_len, E)\n",
    "\n",
    "# --- 3. Image-Grounded Transformer Layer (for Multimodal Encoder/Decoder) ---\n",
    "# This class implements the core logic of a Transformer layer with cross-attention.\n",
    "# It uses built-in MultiheadAttention and Linear layers.\n",
    "class ImageGroundedTransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=nn.GELU(), is_decoder=False):\n",
    "        super().__init__()\n",
    "        self.is_decoder = is_decoder\n",
    "\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout_ffn = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model) # After self-attention\n",
    "        self.norm2 = nn.LayerNorm(d_model) # After cross-attention\n",
    "        self.norm3 = nn.LayerNorm(d_model) # After FFN\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, src, memory, \n",
    "                src_mask=None, memory_mask=None, # attn_mask for MHA\n",
    "                src_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        \n",
    "        # Self-Attention\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "\n",
    "        # Cross-Attention (query from src, key/value from memory)\n",
    "        src2 = self.cross_attn(src, memory, memory, attn_mask=memory_mask, key_padding_mask=memory_key_padding_mask)[0]\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "\n",
    "        # Feed-forward\n",
    "        src3 = self.linear2(self.dropout_ffn(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout3(src3)\n",
    "        src = self.norm3(src)\n",
    "        return src\n",
    "\n",
    "# --- 4. Image-Grounded Text Encoder (for ITM) ---\n",
    "# This part is effectively a BERT encoder with injected cross-attention layers.\n",
    "class ImageGroundedTextEncoder(nn.Module):\n",
    "    def __init__(self, bert_config, num_image_grounded_layers=4):\n",
    "        super().__init__()\n",
    "        # Start with BERT's embeddings\n",
    "        bert_model_base = AutoModel.from_pretrained(\"bert-base-uncased\", config=bert_config)\n",
    "        self.bert_embedding_layer = bert_model_base.embeddings\n",
    "        self.embed_dim = bert_config.hidden_size\n",
    "\n",
    "        # In BLIP, the Image-Grounded Text Encoder re-uses text encoder's weights\n",
    "        # and injects cross-attention. For simplicity and built-in usage,\n",
    "        # we construct custom layers. A more exact BLIP would modify BertModel directly.\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            ImageGroundedTransformerLayer(\n",
    "                self.embed_dim, \n",
    "                bert_config.num_attention_heads, \n",
    "                bert_config.intermediate_size, \n",
    "                bert_config.hidden_dropout_prob,\n",
    "                is_decoder=False # These are encoder-like layers\n",
    "            )\n",
    "            for _ in range(num_image_grounded_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask, image_features):\n",
    "        text_embeddings = self.bert_embedding_layer(input_ids=text_input_ids)\n",
    "        \n",
    "        src_key_padding_mask = (attention_mask == 0) \n",
    "        \n",
    "        hidden_states = text_embeddings\n",
    "        for layer in self.encoder_layers:\n",
    "            hidden_states = layer(\n",
    "                src=hidden_states, \n",
    "                memory=image_features, \n",
    "                src_key_padding_mask=src_key_padding_mask\n",
    "            )\n",
    "        return hidden_states \n",
    "\n",
    "# --- 5. Image-Grounded Text Decoder (for LM) ---\n",
    "# This uses PyTorch's built-in TransformerDecoderLayer directly,\n",
    "# as it naturally supports causal self-attention and cross-attention.\n",
    "class ImageGroundedTextDecoder(nn.Module):\n",
    "    def __init__(self, bert_config, num_image_grounded_layers=4):\n",
    "        super().__init__()\n",
    "        bert_model_base = AutoModel.from_pretrained(\"bert-base-uncased\", config=bert_config)\n",
    "        self.bert_embedding_layer = bert_model_base.embeddings\n",
    "        self.embed_dim = bert_config.hidden_size\n",
    "\n",
    "        # PyTorch's TransformerDecoderLayer inherently has causal self-attention and cross-attention\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=self.embed_dim, \n",
    "            nhead=bert_config.num_attention_heads, \n",
    "            dim_feedforward=bert_config.intermediate_size, \n",
    "            dropout=bert_config.hidden_dropout_prob,\n",
    "            activation=nn.GELU(),\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_image_grounded_layers)\n",
    "        \n",
    "        self.lm_head = nn.Linear(self.embed_dim, bert_config.vocab_size)\n",
    "        # Weight tying (common for language models)\n",
    "        self.lm_head.weight = self.bert_embedding_layer.word_embeddings.weight\n",
    "\n",
    "    def forward(self, text_input_ids, attention_mask, image_features):\n",
    "        text_embeddings = self.bert_embedding_layer(input_ids=text_input_ids)\n",
    "        \n",
    "        seq_len = text_input_ids.shape[1]\n",
    "        # Causal mask for self-attention\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(seq_len).to(text_input_ids.device)\n",
    "        \n",
    "        tgt_key_padding_mask = (attention_mask == 0) # True for padding positions\n",
    "        \n",
    "        hidden_states = self.transformer_decoder(\n",
    "            tgt=text_embeddings,\n",
    "            memory=image_features,\n",
    "            tgt_mask=tgt_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask\n",
    "        )\n",
    "        \n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits # (B, seq_len, vocab_size)\n",
    "\n",
    "\n",
    "# --- BLIP Main Class (Tổng hợp) ---\n",
    "class BLIP(nn.Module):\n",
    "    def __init__(self, image_encoder_params, text_encoder_params, bert_config, \n",
    "                 itm_num_layers=4, lm_num_layers=4, \n",
    "                 add_special_tokens=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Tokenizer setup for special tokens\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(text_encoder_params[\"model_name\"])\n",
    "        if add_special_tokens:\n",
    "            # Add [Encode] and [Decode] tokens to the tokenizer vocabulary\n",
    "            # Check if tokens already exist to avoid adding duplicates\n",
    "            if '[ENC]' not in self.tokenizer.get_vocab():\n",
    "                self.tokenizer.add_special_tokens({'additional_special_tokens': ['[ENC]']})\n",
    "            if '[DEC]' not in self.tokenizer.get_vocab():\n",
    "                self.tokenizer.add_special_tokens({'additional_special_tokens': ['[DEC]']})\n",
    "            self.enc_token_id = self.tokenizer.convert_tokens_to_ids('[ENC]')\n",
    "            self.dec_token_id = self.tokenizer.convert_tokens_to_ids('[DEC]')\n",
    "        else:\n",
    "            self.enc_token_id = None \n",
    "            self.dec_token_id = None\n",
    "\n",
    "        # 1. Khởi tạo các module cơ bản\n",
    "        self.image_encoder = ImageEncoder(**image_encoder_params)\n",
    "        self.embed_dim = image_encoder_params[\"embed_dim\"] # Get embed_dim from params\n",
    "\n",
    "        self.text_encoder_unimodal = TextEncoder(**text_encoder_params)\n",
    "        \n",
    "        # 2. Momentum Encoders (for ITC)\n",
    "        self.image_encoder_m = copy.deepcopy(self.image_encoder)\n",
    "        self.text_encoder_unimodal_m = copy.deepcopy(self.text_encoder_unimodal)\n",
    "\n",
    "        # Freeze momentum encoders (no direct gradient updates)\n",
    "        for param in self.image_encoder_m.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder_unimodal_m.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 3. Image-grounded Text Encoder (cho ITM)\n",
    "        self.image_grounded_text_encoder = ImageGroundedTextEncoder(\n",
    "            bert_config=bert_config,\n",
    "            num_image_grounded_layers=itm_num_layers\n",
    "        )\n",
    "        \n",
    "        # 4. Image-grounded Text Decoder (cho LM)\n",
    "        self.image_grounded_text_decoder = ImageGroundedTextDecoder(\n",
    "            bert_config=bert_config,\n",
    "            num_image_grounded_layers=lm_num_layers\n",
    "        )\n",
    "\n",
    "        # Resize token embeddings for all text models if vocab size changed\n",
    "        if add_special_tokens and len(self.tokenizer) != self.text_encoder_unimodal.bert_model.config.vocab_size:\n",
    "            new_vocab_size = len(self.tokenizer)\n",
    "            self.text_encoder_unimodal.bert_model.resize_token_embeddings(new_vocab_size)\n",
    "            self.text_encoder_unimodal_m.bert_model.resize_token_embeddings(new_vocab_size)\n",
    "            # Ensure weight tying reflects the new embedding layer\n",
    "            # The .bert_embedding_layer in ImageGroundedTextEncoder/Decoder refer to the original BERT's embedding layer,\n",
    "            # so their weights should point to the resized text_encoder_unimodal's embedding weights.\n",
    "            self.image_grounded_text_encoder.bert_embedding_layer.word_embeddings.weight = \\\n",
    "                self.text_encoder_unimodal.bert_model.embeddings.word_embeddings.weight\n",
    "            self.image_grounded_text_decoder.bert_embedding_layer.word_embeddings.weight = \\\n",
    "                self.text_encoder_unimodal.bert_model.embeddings.word_embeddings.weight\n",
    "            self.image_grounded_text_decoder.lm_head.weight = \\\n",
    "                self.text_encoder_unimodal.bert_model.embeddings.word_embeddings.weight # Tie again\n",
    "\n",
    "\n",
    "        # 5. Linear layers for ITC and ITM heads\n",
    "        self.itc_head = nn.Linear(self.embed_dim, self.embed_dim) \n",
    "        self.itm_head = nn.Linear(self.embed_dim, 2) \n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * 0.07) \n",
    "        \n",
    "        # Loss functions\n",
    "        self.cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
    "\n",
    "    # --- Update momentum encoders (used in training loop) ---\n",
    "    @torch.no_grad()\n",
    "    def _momentum_update(self, m=0.995):\n",
    "        \"\"\"Momentum update of the momentum encoder's weights.\"\"\"\n",
    "        for param_q, param_k in zip(self.image_encoder.parameters(), self.image_encoder_m.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "        for param_q, param_k in zip(self.text_encoder_unimodal.parameters(), self.text_encoder_unimodal_m.parameters()):\n",
    "            param_k.data = param_k.data * m + param_q.data * (1. - m)\n",
    "\n",
    "    # --- Encoding functions ---\n",
    "    def unimodal_encode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        image_cls_feature = image_features[:, 0, :] # [CLS] token of image\n",
    "\n",
    "        text_features = self.text_encoder_unimodal(text_input_ids, text_attention_mask)\n",
    "        text_cls_feature = text_features[:, 0, :] # [CLS] token of text\n",
    "\n",
    "        return image_cls_feature, text_cls_feature\n",
    "\n",
    "    @torch.no_grad() \n",
    "    def unimodal_encode_m(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features_m = self.image_encoder_m(images)\n",
    "        image_cls_feature_m = image_features_m[:, 0, :]\n",
    "\n",
    "        text_features_m = self.text_encoder_unimodal_m(text_input_ids, text_attention_mask)\n",
    "        text_cls_feature_m = text_features_m[:, 0, :]\n",
    "        return image_cls_feature_m, text_cls_feature_m\n",
    "\n",
    "    def image_grounded_encode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        \n",
    "        multimodal_representations = self.image_grounded_text_encoder(\n",
    "            text_input_ids, text_attention_mask, image_features\n",
    "        )\n",
    "        \n",
    "        encode_token_representation = multimodal_representations[:, -1, :] \n",
    "        \n",
    "        return encode_token_representation\n",
    "\n",
    "    def image_grounded_decode(self, images, text_input_ids, text_attention_mask):\n",
    "        image_features = self.image_encoder(images) \n",
    "        logits = self.image_grounded_text_decoder(\n",
    "            text_input_ids, text_attention_mask, image_features\n",
    "        )\n",
    "        return logits\n",
    "\n",
    "    # --- Loss Calculation Functions ---\n",
    "    def calculate_itc_loss(self, images, text_input_ids, text_attention_mask):\n",
    "        img_cls, text_cls = self.unimodal_encode(images, text_input_ids, text_attention_mask)\n",
    "        \n",
    "        img_proj = self.itc_head(img_cls)\n",
    "        text_proj = self.itc_head(text_cls)\n",
    "\n",
    "        img_proj = F.normalize(img_proj, dim=-1)\n",
    "        text_proj = F.normalize(text_proj, dim=-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self._momentum_update() \n",
    "            img_cls_m, text_cls_m = self.unimodal_encode_m(images, text_input_ids, text_attention_mask)\n",
    "            img_proj_m = F.normalize(self.itc_head(img_cls_m), dim=-1)\n",
    "            text_proj_m = F.normalize(self.itc_head(text_cls_m), dim=-1)\n",
    "\n",
    "        logits_per_image = torch.matmul(img_proj, text_proj_m.T) / self.temperature\n",
    "        logits_per_text = torch.matmul(text_proj, img_proj_m.T) / self.temperature \n",
    "\n",
    "        labels = torch.arange(logits_per_image.shape[0], device=logits_per_image.device) \n",
    "        \n",
    "        loss_i = self.cross_entropy_loss(logits_per_image, labels)\n",
    "        loss_t = self.cross_entropy_loss(logits_per_text, labels)\n",
    "        \n",
    "        loss = (loss_i + loss_t) / 2\n",
    "        return loss\n",
    "\n",
    "    def calculate_itm_loss(self, images, text_input_ids, text_attention_mask, labels):\n",
    "        multimodal_representation = self.image_grounded_encode(images, text_input_ids, text_attention_mask)\n",
    "        logits = self.itm_head(multimodal_representation) \n",
    "        \n",
    "        loss = self.cross_entropy_loss(logits, labels)\n",
    "        return loss\n",
    "\n",
    "    def calculate_lm_loss(self, images, text_input_ids, text_attention_mask, labels):\n",
    "        logits = self.image_grounded_decode(images, text_input_ids, text_attention_mask)\n",
    "        \n",
    "        loss = self.cross_entropy_loss(logits.view(-1, logits.shape[-1]), labels.view(-1))\n",
    "        return loss\n",
    "\n",
    "    def forward(self, images, \n",
    "                text_input_ids_itc=None, text_attention_mask_itc=None,\n",
    "                text_input_ids_itm=None, text_attention_mask_itm=None, itm_labels=None,\n",
    "                text_input_ids_lm=None, text_attention_mask_lm=None, lm_labels=None,\n",
    "                task_type=\"pretrain\"):\n",
    "        \n",
    "        total_loss = 0\n",
    "        losses_dict = {}\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"itc\":\n",
    "            itc_loss = self.calculate_itc_loss(images, text_input_ids_itc, text_attention_mask_itc)\n",
    "            total_loss += itc_loss\n",
    "            losses_dict[\"itc_loss\"] = itc_loss\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"itm\":\n",
    "            itm_loss = self.calculate_itm_loss(images, text_input_ids_itm, text_attention_mask_itm, itm_labels)\n",
    "            total_loss += itm_loss\n",
    "            losses_dict[\"itm_loss\"] = itm_loss\n",
    "\n",
    "        if task_type == \"pretrain\" or task_type == \"lm\":\n",
    "            lm_loss = self.calculate_lm_loss(images, text_input_ids_lm, text_attention_mask_lm, lm_labels)\n",
    "            total_loss += lm_loss\n",
    "            losses_dict[\"lm_loss\"] = lm_loss\n",
    "            \n",
    "        if task_type == \"pretrain\":\n",
    "            return total_loss, losses_dict\n",
    "        elif task_type in [\"itc\", \"itm\", \"lm\"]:\n",
    "            if task_type == \"itc\": return itc_loss\n",
    "            if task_type == \"itm\": return itm_loss\n",
    "            if task_type == \"lm\": return lm_loss\n",
    "        elif task_type == \"inference\":\n",
    "            raise NotImplementedError(\"Inference (e.g., text generation) not implemented in forward. Use .generate() method.\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task_type: {task_type}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, images, max_length=20, num_beams=1, temperature=1.0):\n",
    "        self.eval() \n",
    "        \n",
    "        batch_size = images.shape[0]\n",
    "        device = images.device\n",
    "\n",
    "        input_ids = torch.full((batch_size, 1), self.dec_token_id, dtype=torch.long, device=device)\n",
    "        attention_mask = torch.ones(batch_size, 1, dtype=torch.long, device=device)\n",
    "\n",
    "        image_features = self.image_encoder(images)\n",
    "\n",
    "        generated_ids = []\n",
    "        for _ in range(max_length):\n",
    "            logits = self.image_grounded_decode(input_ids=input_ids, attention_mask=attention_mask, image_features=image_features)\n",
    "            \n",
    "            next_token_logits = logits[:, -1, :] / temperature\n",
    "\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            generated_ids.append(next_token)\n",
    "            \n",
    "            input_ids = torch.cat([input_ids, next_token.unsqueeze(-1)], dim=-1)\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((batch_size, 1), dtype=torch.long, device=device)], dim=-1)\n",
    "\n",
    "            # Check if all sequences have generated EOS or SEP token (BERT usually uses SEP for EOS)\n",
    "            if torch.all(next_token == self.tokenizer.sep_token_id) or torch.all(next_token == self.tokenizer.pad_token_id):\n",
    "                 break\n",
    "        \n",
    "        generated_ids = torch.stack(generated_ids, dim=1)\n",
    "        # Decode, skipping special tokens like [CLS], [SEP], [PAD], [ENC], [DEC]\n",
    "        generated_texts = [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "        \n",
    "        self.train() \n",
    "        return generated_texts\n",
    "\n",
    "# --- Ví dụ sử dụng ---\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    image_encoder_params = {\n",
    "        \"image_size\": 224,\n",
    "        \"patch_size\": 16,\n",
    "        \"embed_dim\": 768, # ViT-Base dimension\n",
    "        \"num_layers\": 12, # Number of Encoder blocks in ViT\n",
    "        \"num_heads\": 12\n",
    "    }\n",
    "    \n",
    "    text_encoder_params = {\n",
    "        \"model_name\": \"bert-base-uncased\"\n",
    "    }\n",
    "    \n",
    "    bert_config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    blip_model = BLIP(\n",
    "        image_encoder_params=image_encoder_params,\n",
    "        text_encoder_params=text_encoder_params,\n",
    "        bert_config=bert_config,\n",
    "        itm_num_layers=4, \n",
    "        lm_num_layers=4,  \n",
    "        add_special_tokens=True \n",
    "    ).to(device)\n",
    "    \n",
    "    tokenizer = blip_model.tokenizer\n",
    "    enc_token_id = tokenizer.convert_tokens_to_ids('[ENC]')\n",
    "    dec_token_id = tokenizer.convert_tokens_to_ids('[DEC]')\n",
    "    print(f\"Tokenizer vocab size: {len(tokenizer)}\")\n",
    "    print(f\"[ENC] token ID: {enc_token_id}\")\n",
    "    print(f\"[DEC] token ID: {dec_token_id}\")\n",
    "    print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "    print(f\"SEP token ID (often used as EOS for BERT): {tokenizer.sep_token_id}\")\n",
    "\n",
    "    blip_model.eval() \n",
    "    print(\"BLIP Model initialized successfully!\")\n",
    "    print(f\"Total parameters: {sum(p.numel() for p in blip_model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    # --- Tạo dữ liệu dummy ---\n",
    "    batch_size = 2\n",
    "    dummy_images = torch.randn(batch_size, 3, 224, 224).to(device) \n",
    "    dummy_texts = [\"a little girl holding a kitten next to a blue fence\", \"a dog running in a park\"]\n",
    "    \n",
    "    max_seq_len = 50\n",
    "\n",
    "    # For ITC (standard text input)\n",
    "    encoded_itc = tokenizer(dummy_texts, padding=\"max_length\", truncation=True, max_length=max_seq_len, return_tensors=\"pt\")\n",
    "    text_input_ids_itc = encoded_itc.input_ids.to(device)\n",
    "    text_attention_mask_itc = encoded_itc.attention_mask.to(device)\n",
    "\n",
    "    # For ITM ([CLS] text [SEP] [ENC])\n",
    "    # The [ENC] token is appended. Max length adjusts for it.\n",
    "    encoded_itm_base = tokenizer(dummy_texts, padding=\"max_length\", truncation=True, max_length=max_seq_len-1, return_tensors=\"pt\")\n",
    "    text_input_ids_itm = torch.cat([encoded_itm_base.input_ids, torch.full((batch_size, 1), enc_token_id, dtype=torch.long)], dim=1).to(device)\n",
    "    text_attention_mask_itm = torch.cat([encoded_itm_base.attention_mask, torch.ones((batch_size, 1), dtype=torch.long)], dim=1).to(device)\n",
    "    itm_labels = torch.tensor([1, 0], dtype=torch.long).to(device) # 1 matched, 0 unmatched\n",
    "\n",
    "    # For LM ([DEC] text_prefix_for_decoder_input and text_full_for_labels)\n",
    "    # Decoder input will be [DEC] + text_tokens_shifted_right (e.g., [DEC] T1 T2 T3)\n",
    "    # Labels will be text_tokens (e.g., T1 T2 T3 [SEP/PAD])\n",
    "    \n",
    "    # Text for decoder input: Prepend [DEC]\n",
    "    lm_input_texts = [f\"{tokenizer.decode(dec_token_id, skip_special_tokens=True)} {t}\" for t in dummy_texts]\n",
    "    encoded_lm_input = tokenizer(lm_input_texts, padding=\"max_length\", truncation=True, max_length=max_seq_len, return_tensors=\"pt\")\n",
    "    text_input_ids_lm = encoded_lm_input.input_ids.to(device)\n",
    "    text_attention_mask_lm = encoded_lm_input.attention_mask.to(device)\n",
    "\n",
    "    # Labels for LM loss (shifted by one, padding masked to -100)\n",
    "    # Original target tokens: CLS T1 T2 T3 SEP PAD PAD\n",
    "    # Labels should be: T1 T2 T3 SEP PAD PAD PAD (shifted left, first CLS removed)\n",
    "    # This means input_ids to decoder are what we give.\n",
    "    # labels are what we expect to predict, one token after the input.\n",
    "    \n",
    "    # We will use text_input_ids_lm as the input to the decoder.\n",
    "    # And then compute labels by shifting it.\n",
    "    lm_labels = text_input_ids_lm.clone()\n",
    "    lm_labels = torch.cat([lm_labels[:, 1:], torch.full((batch_size, 1), tokenizer.pad_token_id, dtype=torch.long, device=device)], dim=1)\n",
    "    lm_labels[text_attention_mask_lm == 0] = -100 # Mask padding\n",
    "\n",
    "    print(\"\\n--- Testing Unimodal Encoding (for ITC) ---\")\n",
    "    with torch.no_grad():\n",
    "        img_cls, text_cls = blip_model.unimodal_encode(dummy_images, text_input_ids_itc, text_attention_mask_itc)\n",
    "    print(f\"Image CLS feature shape: {img_cls.shape}\") \n",
    "    print(f\"Text CLS feature shape: {text_cls.shape}\") \n",
    "\n",
    "    print(\"\\n--- Testing Image-Grounded Encoding (for ITM) ---\")\n",
    "    with torch.no_grad():\n",
    "        itm_output = blip_model.image_grounded_encode(dummy_images, text_input_ids_itm, text_attention_mask_itm)\n",
    "    print(f\"Image-Grounded Encode (ITM) output shape: {itm_output.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing Image-Grounded Decoding (for LM) ---\")\n",
    "    with torch.no_grad():\n",
    "        lm_logits = blip_model.image_grounded_decode(dummy_images, text_input_ids_lm, text_attention_mask_lm)\n",
    "    print(f\"Image-Grounded Decode (LM) logits shape: {lm_logits.shape}\")\n",
    "\n",
    "    print(\"\\n--- Testing Full Pretrain Forward Pass (Train Mode) ---\")\n",
    "    blip_model.train() \n",
    "    total_loss, losses_dict = blip_model.forward(\n",
    "        images=dummy_images,\n",
    "        text_input_ids_itc=text_input_ids_itc,\n",
    "        text_attention_mask_itc=text_attention_mask_itc,\n",
    "        text_input_ids_itm=text_input_ids_itm,\n",
    "        text_attention_mask_itm=text_attention_mask_itm,\n",
    "        itm_labels=itm_labels,\n",
    "        text_input_ids_lm=text_input_ids_lm, \n",
    "        text_attention_mask_lm=text_attention_mask_lm,\n",
    "        lm_labels=lm_labels, \n",
    "        task_type=\"pretrain\"\n",
    "    )\n",
    "    print(f\"Total Pretrain Loss: {total_loss.item():.4f}\")\n",
    "    print(f\"Individual Losses: {losses_dict}\")\n",
    "\n",
    "    print(\"\\n--- Testing Inference (Generation) ---\")\n",
    "    generated_captions = blip_model.generate(dummy_images, max_length=20)\n",
    "    print(\"Generated Captions:\")\n",
    "    for i, caption in enumerate(generated_captions):\n",
    "        print(f\"Image {i+1}: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e99332",
   "metadata": {},
   "source": [
    "### Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bb38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, image_size = 224, patch_size = 16, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        num_patch = (image_size // patch_size) ** 2\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "natmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
